---
title: Variable Elimination and Junction Trees
layout: post
---

In this post, we are going to talk about two techniques in performing exact inference over a graph: **variable elimination** and **junction trees**. We will see that variable elimination can be used to compute marginal probabilities, and that junction trees are a formalized way of building a data structure that can be used to perform variable elimination [1]. 

## Variable Elimination
We can use variable elimination to compute marginal probabilities from a joint probability represented as a graph. For example, we may want to compute $$P(X_1, X_3)$$ from the graph below which describes a joint distribution that can be written as [8]: 

$$P(X_1, X_3) = \sum_{X_2, X_4, X_5} P(X_1 | X_2) P(X_2) P(X_3 | X_1) P(X_4 | X_1, X_2) P(X_5 | X_2, X_4)$$

(Note that the meaning of these factors varies slightly depending on which type of graphical model you are working with. For DGMs, the factors are conditional probabilities with respect to the node’s parents, i.e. [8]:

$$\{ P(X_i | X_{PA_i}) \}$$

For UGMs, the factors are called **clique potentials** and are written as $$\phi_c (X_c)$$. Clique potentials become probabilities after they are normalized [8].)

![Fig 1]({{ site.baseurl }}/images/2021-10-20-VE-JT-fig1.png "Figure 1"){:width=75%}   
Figure 1   

The reason why variable elimination is so powerful is that it greatly speeds up the computation of this marginal probability. In an [earlier post](https://sassafras13.github.io/Graphs/), I noted that graphical models are computationally efficient because they allow us to write all of the possible probabilities more compactly than in a tabular format. However, there is a limitation to this: when we need to sum up (or marginalize) over certain random variables in the joint distribution, that can still be prohibitively expensive even though we are working with a graphical model that is supposed to be efficient. Variable elimination helps us to speed up the process of marginalizing over random variables in our model by side-stepping the need to compute the sum as written above [8].

So now that we have a sense of why variable elimination is useful, let’s see how we can apply it to the example above. Let’s begin by noting that we could break out the summation over $$X_2, X_4, X_5$$ into individual summations [8]: 

$$P(X_1, X_3) = \sum_{X_2} \sum_{X_4} \sum_{X_5} P(X_1 | X_2) P(X_2) P(X_3 | X_1) P(X_4 | X_1, X_2) P(X_5 | X_2, X_4)$$

And we can move the summations to apply to only the factors that contain those variables within that scope [8]: 

$$P(X_1, X_3) = P(X_3 | X_1) \sum_{X_2} P(X_1 | X_2) P(X_2) \sum_{X_4} P(X_4 | X_1, X_2) \sum_{X_5} P(X_5 | X_2, X_4)$$

Now I can rewrite my final term as a message function that depends on $$X_2, X_4$$, i.e. $$m_5(X_2, X_4)$$. (Note that it does not depend on $$X_5$$ because it is the conditional probability for $$X_5$$, dependent on $$X_2, X_4$$.) I can now substitute in the message $$m_5$$ and continue to repeat this process until I have eliminated all of the variables except the ones I’m interested in [8]. 

$$P(X_1, X_3) = P(X_3 | X_1) \sum_{X_2} P(X_1 | X_2) P(X_2) \sum_{X_4} P(X_4 | X_1, X_2) m_5(X_2, X_4)$$

$$P(X_1, X_3) = P(X_3 | X_1) \sum_{X_2} P(X_1 | X_2) P(X_2) m_4(X_1, X_2)$$

$$P(X_1, X_3) = P(X_3 | X_1) m_2(X_1)$$

Variable elimination has a couple of associated properties [8]: 

* **Distributive property**: This describes how we can distribute the summing process used in variable elimination [2].   

$$\phi(X) = \phi_1(X) \cdot \phi_2(X)$$

$$\sum_{X_s} \phi$$ and $$X_s \notin \text{scope}(\phi_1)$$  

Then $$\sum_{X_s} \phi_1 \phi_2 = \phi_1 \sum_{X_s}\phi_2$$  

* **Factor marginalization**: This describes how we can marginalize over factors with variable elimination [2].  

$$\Psi(X - \{Y\}) = \sum_Y \phi(X)$$

There is one more idea related to variable elimination that is important to talk about here, and that is the idea of **treewidth** [2]. 

### Treewidth

The treewidth for the variable elimination induced graph is important for calculating the computational complexity of variable elimination for a given graph [2]. We will define it in a moment, but first it is important to explain that the scope of each intermediate factor (or message, like $$m_5$$ in our example above) is a clique*1 in the induced graph for our system. This should make sense because the only reason that we were able to write a single message function for those random variables was because they were all adjacent to each other. Moreover, the **maximal clique** *1 for the induced graph is also by default one of the intermediate factors that will be written during the process of variable elimination [8]. 

Now that we understand that the intermediate factors in variable elimination are related to cliques in the induced graph, we can define the treewidth. The treewidth of a graph G is the size of the largest clique of the graph, minus 1. That is [2]:

$$\text{treewidth}(G) = \min_{\sigma \in \mathcal{S}_p} \text{width}(G_{X_{\sigma}})$$

Where $$X_{\sigma}$$ is a specific order of all the random variables in G according to permutation $$\sigma$$.The set of all possible permutations of $$p$$ elements is $$\mathcal{S_p}$$ [2]. 

Note that in order to obtain the largest clique via variable elimination, you have to know the best possible order in which to eliminate the variables. Since our goal is to minimize computational complexity with variable elimination, the best possible ordering is one which generates the **smallest** maximal clique possible [2]. 

## Junction Trees
Let’s start with a couple of definitions to work our way to a junction tree. We start with an undirected graph, G. This undirected graph has a **clique graph** $$\mathcal{H} = (\mathcal{V}, \mathcal{E})$$. Each node $$\mathcal{V}$$ is a maximal clique in G. Furthermore, any edge between two cliques in $$\mathcal{H}$$ indicates that there is a non-null intersection between them, that is, for $$(C_1, C_2) \in \mathbb{E}(\mathcal{H})$$,  $$\exists C_1 \cap C_2 \neq 0$$. Each of these edges is also associated with a separation-set, or “sep-set”, written as $$S_{12} = C_1 \cap C_2$$. Finally, we can say that if the graph has no cycles and just one connected component, then we call it a **clique tree**, and if it has multiple connected components then it is a **clique forest** [1]. 

This is kind of a lot to take in, but basically so far we have just said that we can think of any UG graph as being a graph made up of maximal cliques that are connected via edges. Now we’re going to focus on clique trees because they have lots of interesting properties. The first cool property is called the **running intersection property**, which states that  a clique tree, $$\mathcal{T} = (\mathcal{V}_T, \mathcal{E}_T)$$, possesses this property if there is a variable, X, that is common to two nodes (i.e. two maximal cliques $$C_i, C_j \in \mathcal{V}_T$$ share X $$\in C_i \cup C_j$$) that also occurs in all of the nodes in the _unique_ path between $$C_i$$ and $$C_j$$ in $$\mathcal{T}$$ [1]. 

Now we get to the crux of this section: any clique tree that satisfies the running intersection property is called a **junction tree**. Not all clique trees are junction trees, but it can be shown that every chordal UG has a junction tree [1]. 

A junction tree can be constructed from a general UG G in 3 steps [1]: 

**1. Triangulate the graph.** This produces a chordal graph which is now guaranteed to have a junction tree.   

**2. Find the maximal cliques.** This can be NP-hard in general.   

**3. Connect the maximal cliques with edges to form a tree.** First connect maximal cliques that share variables with an edge. The weight of the edge is computed as the number of shared variables. From these edges, find the maximum spanning tree. The maximum spanning tree*2 is the junction tree for this graph, G.   

## Footnotes:
*1 A clique is a subset of vertices in an undirected graph where every two vertices are adjacent to each other [3]. A maximal clique, specifically, is a clique that cannot be extended by including another adjacent vertex [4]. 

*2 A maximum spanning tree is a spanning tree that has equal to or greater than weights as compared to all of the other spanning trees for the given graph [5]. And a spanning tree is simply a subgraph of the graph G that is a tree which includes all of the vertices in the graph G [6]. And what is a tree, you ask? A tree is a UG where any two of the vertices are connected by exactly one path. Another way to call a tree is to say it is a “connected acyclic undirected graph” [7]. 

## References:

[1] Ravikumar, P. “Exact Inference: Junction Trees.” 10-708: Probabilistic Graphical Models. 2021. Class notes. 

[2] Ravikumar, P. “Exact Inference: Variable Elimination.” 10-708: Probabilistic Graphical Models. 2021. Class notes. 

[3] “Clique.” Wikipedia. <https://en.wikipedia.org/wiki/Clique_(graph_theory)>. Visited 18 Oct 2021. 

[4] Weisstein, Eric W. "Maximal Clique." From MathWorld--A Wolfram Web Resource. <https://mathworld.wolfram.com/MaximalClique.html>. Visited 18 Oct 2021. 

[5] “Minimum spanning tree.” Wikipedia. <https://en.wikipedia.org/wiki/Minimum_spanning_tree>. Visited 18 Oct 2021. 

[6] “Spanning tree.” Wikipedia. <https://en.wikipedia.org/wiki/Spanning_tree>. Visited 18 Oct 2021. 

[7] “Tree (graph theory).” Wikipedia. <https://en.wikipedia.org/wiki/Tree_(graph_theory)>. Visited 18 Oct 2021. 

[8] “Recitation 3: Variable Elimination Questions.” 15 Oct 2021. 10-708: Probabilistic Graphical Models. 2021. Class notes.
